import QuizEmbed from '@/components/QuizEmbed';
import ProgressTracker from '@/components/ProgressTracker';

# Module 1: Prompt Fundamentals

<ProgressTracker moduleId="module-1" />

> "If you wish to make an apple pie from scratch, you must first invent the universe." — Carl Sagan

**Module Duration:** 45 minutes  
**Source Material:** [davidkimai/Context-Engineering](https://github.com/davidkimai/Context-Engineering) - 00_foundations/01_atoms_prompting.md

---

## The Atom: A Single Instruction

In our journey through context engineering, we begin with the most fundamental unit: the **atom** — a single, standalone instruction to an LLM.

```
┌───────────────────────────────────────────────┐
│                                               │
│  "Write a poem about the ocean in 4 lines."   │
│                                               │
└───────────────────────────────────────────────┘
```

This is prompt engineering in its purest form: one human, one instruction, one model response. Simple, direct, atomic.

## The Anatomy of an Atomic Prompt

Let's break down what makes an effective atomic prompt:

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│  ATOMIC PROMPT = [TASK] + [CONSTRAINTS] + [OUTPUT FORMAT]   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

For example:

```
┌─────────────────────┬────────────────────────┬────────────────────┐
│        TASK         │      CONSTRAINTS       │   OUTPUT FORMAT    │
├─────────────────────┼────────────────────────┼────────────────────┤
│ "Write a poem       │ "about the ocean       │ "in 4 lines."      │
│  about space."      │  using only words      │                    │
│                     │  with 5 letters        │                    │
│                     │  or less."             │                    │
└─────────────────────┴────────────────────────┴────────────────────┘
```

<QuizEmbed 
  moduleId="module-1" 
  section="anatomy"
  quizzes={[
    {
      difficulty: 'simple',
      question: 'According to the anatomy of an atomic prompt, what are the three essential components?',
      options: {
        A: 'Input, Processing, Output',
        B: 'Task, Constraints, Output Format',
        C: 'Question, Context, Answer',
        D: 'Instruction, Example, Response'
      },
      correct: 'B',
      hint: 'Look at the formula shown in the box diagram.',
      explanation: 'An atomic prompt consists of three core elements: TASK (what to do), CONSTRAINTS (boundaries or requirements), and OUTPUT FORMAT (how the result should be structured).'
    },
    {
      difficulty: 'medium',
      question: 'Why is the term "atomic" used to describe these basic prompts?',
      options: {
        A: 'They are extremely small in size',
        B: 'They represent the fundamental, indivisible unit of LLM interaction',
        C: 'They are related to atomic theory in physics',
        D: 'They work at the molecular level'
      },
      correct: 'B',
      hint: 'Think about the definition of "atom" as a fundamental building block.',
      explanation: 'The term "atomic" is used because these prompts represent the most basic, fundamental unit of LLM interaction—like atoms in chemistry, they are the simplest building blocks from which more complex structures are built.'
    },
    {
      difficulty: 'hard',
      question: 'In the poem example with constraints "using only words with 5 letters or less," what aspect of prompt engineering is being demonstrated?',
      options: {
        A: 'Output format specification',
        B: 'Task definition clarity',
        C: 'Constraint-based prompt design to reduce ambiguity',
        D: 'Few-shot learning technique'
      },
      correct: 'C',
      hint: 'Consider how adding specific constraints affects the model\'s response space.',
      explanation: 'This demonstrates constraint-based prompt design—by adding specific limitations, we reduce ambiguity and guide the model toward more predictable, controlled outputs within defined boundaries.'
    }
  ]}
/>

## The Limitations of Atoms

While atomic prompts are the building blocks of LLM interactions, they quickly reveal fundamental limitations:

```
┌──────────────────────────────────────┐
│ LIMITATIONS OF ATOMIC PROMPTS        │
├──────────────────────────────────────┤
│ ✗ No memory across interactions      │
│ ✗ Limited demonstration capability   │
│ ✗ No complex reasoning scaffolds     │
│ ✗ Prone to ambiguity                 │
│ ✗ High variance in outputs           │
└──────────────────────────────────────┘
```

Let's measure this empirically with a simple experiment:

```python
# A basic atomic prompt
atomic_prompt = "List 5 symptoms of diabetes."

# Send to LLM multiple times
responses = [llm.generate(atomic_prompt) for _ in range(5)]

# Measure variability
unique_symptoms = set()
for response in responses:
    symptoms = extract_symptoms(response)
    unique_symptoms.update(symptoms)

print(f"Found {len(unique_symptoms)} unique symptoms across 5 identical prompts")
# Typically outputs far more than just 5 unique symptoms
```

The problem? Models struggle with consistency when given minimal context.

<QuizEmbed 
  moduleId="module-1" 
  section="limitations"
  quizzes={[
    {
      difficulty: 'simple',
      question: 'Which of the following is NOT listed as a limitation of atomic prompts?',
      options: {
        A: 'No memory across interactions',
        B: 'High variance in outputs',
        C: 'Unable to process natural language',
        D: 'Limited demonstration capability'
      },
      correct: 'C',
      hint: 'All atomic prompts can process natural language—that\'s their basic function.',
      explanation: 'Atomic prompts CAN process natural language; that\'s their fundamental capability. The actual limitations include lack of memory, high variance, no reasoning scaffolds, and limited demonstration ability.'
    },
    {
      difficulty: 'medium',
      question: 'In the diabetes symptoms experiment, why does the model produce more than 5 unique symptoms across 5 runs?',
      options: {
        A: 'The model is malfunctioning',
        B: 'High variance in outputs due to minimal context leads to inconsistency',
        C: 'The prompt is asking for different symptoms each time',
        D: 'The model is trying to be helpful by providing variety'
      },
      correct: 'B',
      hint: 'Think about what happens when the same prompt produces different results.',
      explanation: 'This demonstrates high output variance—without sufficient context or constraints, atomic prompts lead to inconsistent responses. The model interprets the same prompt differently each time, resulting in different symptom selections.'
    },
    {
      difficulty: 'hard',
      question: 'The limitation "no complex reasoning scaffolds" means that atomic prompts:',
      options: {
        A: 'Cannot perform any logical reasoning',
        B: 'Lack structured frameworks for multi-step reasoning processes',
        C: 'Are unable to understand complex questions',
        D: 'Cannot generate detailed explanations'
      },
      correct: 'B',
      hint: 'Think about what "scaffold" means in education—a support structure for learning.',
      explanation: 'Reasoning scaffolds are structured frameworks that break down complex problems into steps. Atomic prompts lack these multi-step structures, making them ineffective for tasks requiring sequential reasoning or chain-of-thought processes.'
    }
  ]}
/>

## The Single-Atom Baseline: Useful But Limited

Despite their limitations, atomic prompts establish our baseline. They help us:

1. Measure token efficiency (minimal overhead)
2. Benchmark response quality
3. Establish a control for experiments

```
                     [Response Quality]
                            ▲
                            │
                            │               ⭐ Context
                            │                 Engineering
                            │               
                            │           
                            │       ⭐ Advanced
                            │         Prompting
                            │
                            │   ⭐ Basic Prompting
                            │
                            │
                            └────────────────────────►
                                  [Complexity]
```

## The Unspoken Context: What Models Already "Know"

Even with atomic prompts, LLMs leverage massive implicit context from their training:

```
┌───────────────────────────────────────────────────────────────┐
│ IMPLICIT CONTEXT IN MODELS                                    │
├───────────────────────────────────────────────────────────────┤
│ ✓ Language rules and grammar                                  │
│ ✓ Common knowledge facts                                      │
│ ✓ Format conventions (lists, paragraphs, etc.)                │
│ ✓ Domain-specific knowledge (varies by model)                 │
│ ✓ Learned interaction patterns                                │
└───────────────────────────────────────────────────────────────┘
```

This implicit knowledge gives us a foundation, but it's unreliable and varies between models and versions.

<QuizEmbed 
  moduleId="module-1" 
  section="implicit-context"
  quizzes={[
    {
      difficulty: 'simple',
      question: 'What is "implicit context" in LLMs?',
      options: {
        A: 'Context that users must provide explicitly in prompts',
        B: 'Knowledge and patterns the model learned during training',
        C: 'Hidden instructions programmed by developers',
        D: 'Context that appears between the lines of text'
      },
      correct: 'B',
      hint: 'Think about what the model brings to the interaction without being told.',
      explanation: 'Implicit context refers to the vast knowledge, patterns, and capabilities that LLMs acquired during training. This includes language rules, common facts, format conventions, and interaction patterns—all available without explicit instruction.'
    },
    {
      difficulty: 'medium',
      question: 'Why is implicit context described as "unreliable"?',
      options: {
        A: 'It contains factual errors from training data',
        B: 'It varies between models, versions, and can be inconsistently applied',
        C: 'It disappears after a few interactions',
        D: 'It only works in English'
      },
      correct: 'B',
      hint: 'Consider how different models might respond differently to the same prompt.',
      explanation: 'Implicit context is unreliable because it varies between different models and versions, and can be inconsistently accessed or applied. You can\'t guarantee which implicit knowledge will be activated for any given prompt.'
    },
    {
      difficulty: 'hard',
      question: 'How does implicit context relate to the need for context engineering?',
      options: {
        A: 'Implicit context makes context engineering unnecessary',
        B: 'Context engineering replaces all implicit context',
        C: 'Context engineering provides explicit structure to compensate for unreliable implicit context',
        D: 'Implicit context and context engineering are unrelated concepts'
      },
      correct: 'C',
      hint: 'Think about why we need to engineer context if models already "know" things.',
      explanation: 'Context engineering exists precisely because implicit context is unreliable. By providing explicit structure, examples, and constraints, we compensate for the variability of implicit knowledge and gain more control over model behavior.'
    }
  ]}
/>

## The Power Law: Token-Quality Curve

For many tasks, we observe a power law relationship between context tokens and output quality:

```
Quality
      ▲
      │                        •
      │                    •       •
      │                •               •
      │            •                       •
      │        •                               •
      │    •
      │•
      └───────────────────────────────────────────► Tokens
          [Poor Start]  [Maximum ROI]  [Diminishing Returns]
```

The critical insight: there's a "maximum ROI zone" where adding just a few tokens yields dramatic quality improvements as well as "diminishing returns", where adding more tokens instead degrades performance.

### [Read more on Context Rot](https://research.trychroma.com/context-rot)

<QuizEmbed 
  moduleId="module-1" 
  section="power-law"
  quizzes={[
    {
      difficulty: 'simple',
      question: 'According to the Token-Quality Curve, what happens in the "Maximum ROI" zone?',
      options: {
        A: 'Adding tokens has no effect on quality',
        B: 'Each additional token yields significant quality improvements',
        C: 'Quality decreases as tokens increase',
        D: 'Token count becomes irrelevant'
      },
      correct: 'B',
      hint: 'ROI stands for "Return on Investment"—think about where you get the most value.',
      explanation: 'The Maximum ROI zone is where adding tokens provides the highest value—each additional token yields substantial quality improvements. This is the sweet spot of context engineering where your token investment pays off most.'
    },
    {
      difficulty: 'medium',
      question: 'What is "Context Rot" as referenced in the power law discussion?',
      options: {
        A: 'When context becomes outdated over time',
        B: 'Performance degradation from adding too much context',
        C: 'Memory corruption in long conversations',
        D: 'When models forget previous instructions'
      },
      correct: 'B',
      hint: 'Look at the right side of the curve—what happens in the "Diminishing Returns" zone?',
      explanation: 'Context Rot refers to the phenomenon in the "Diminishing Returns" zone where adding more tokens actually degrades performance. Too much context can confuse the model, dilute important information, or introduce noise.'
    },
    {
      difficulty: 'hard',
      question: 'The power law relationship between tokens and quality suggests that optimal context engineering should:',
      options: {
        A: 'Always maximize token count to ensure complete information',
        B: 'Use minimal tokens to avoid any potential confusion',
        C: 'Find the efficient frontier where quality gains justify token costs',
        D: 'Distribute tokens evenly across all prompt components'
      },
      correct: 'C',
      hint: 'Think about balancing the trade-offs between token cost and quality improvement.',
      explanation: 'Optimal context engineering means finding the efficient frontier—the point where quality gains justify the token costs, typically in the Maximum ROI zone. This requires balancing comprehensiveness against the risks of context rot and resource consumption.'
    }
  ]}
/>

## From Atoms to Molecules: The Need for More Context

The limitations of atoms lead us naturally to our next step: **molecules**, or multi-part prompts that combine instructions with examples, additional context, and structured formats.

Here's the fundamental transition:

```
┌──────────────────────────┐         ┌──────────────────────────┐
│                          │         │ "Here's an example:      │
│ "Write a limerick about  │    →    │  There once was a...     │
│  a programmer."          │         │                          │
│                          │         │  Now write a limerick    │
└──────────────────────────┘         │  about a programmer."    │
                                     └──────────────────────────┘
    [Atomic Prompt]                       [Molecular Prompt]
```

By adding examples and structure, we begin to shape the context window deliberately—the first step toward context engineering.

## Measuring Atom Efficiency: Your First Task

Before moving on, try this simple exercise:

1. Take a basic task you'd give to an LLM
2. Create three different atomic prompt versions
3. Measure tokens used and subjective quality
4. Plot the efficiency frontier

```
┌─────────────────────────────────────────────────────────────┐
│ Task: Summarize a news article                              │
├─────────┬───────────────────────────────┬────────┬──────────┤
│ Version │ Prompt                        │ Tokens │ Quality  │
├─────────┼───────────────────────────────┼────────┼──────────┤
│ A       │ "Summarize this article."     │ 4      │ 2/10     │
├─────────┼───────────────────────────────┼────────┼──────────┤
│ B       │ "Provide a concise summary    │ 14     │ 6/10     │
│         │  of this article in 3         │        │          │
│         │  sentences."                  │        │          │
├─────────┼───────────────────────────────┼────────┼──────────┤
│ C       │ "Write a summary of the key   │ 27     │ 8/10     │
│         │  points in this article,      │        │          │
│         │  highlighting the main        │        │          │
│         │  people and events."          │        │          │
└─────────┴───────────────────────────────┴────────┴──────────┘
```

<QuizEmbed 
  moduleId="module-1" 
  section="efficiency"
  quizzes={[
    {
      difficulty: 'simple',
      question: 'In the article summarization example, which version demonstrates the best quality-to-token ratio?',
      options: {
        A: 'Version A (4 tokens, 2/10 quality)',
        B: 'Version B (14 tokens, 6/10 quality)',
        C: 'Version C (27 tokens, 8/10 quality)',
        D: 'All versions have equal efficiency'
      },
      correct: 'B',
      hint: 'Consider quality per token: A=0.5, B=0.43, C=0.30',
      explanation: 'Version B offers the best efficiency at ~0.43 quality points per token. While Version C has higher absolute quality, it requires nearly double the tokens for only 33% more quality—demonstrating diminishing returns.'
    },
    {
      difficulty: 'medium',
      question: 'Why does adding "in 3 sentences" to Version B improve quality compared to Version A?',
      options: {
        A: 'It makes the prompt longer',
        B: 'It provides explicit output format constraint, reducing ambiguity',
        C: 'It uses more sophisticated vocabulary',
        D: 'It activates special summarization mode'
      },
      correct: 'B',
      hint: 'Recall the anatomy of atomic prompts: Task + Constraints + Output Format',
      explanation: 'Adding "in 3 sentences" provides an explicit output format constraint, which reduces ambiguity about what kind of summary is expected. This aligns with the atomic prompt anatomy: Task (summarize) + Output Format (3 sentences).'
    },
    {
      difficulty: 'hard',
      question: 'The efficiency frontier concept suggests that Version C might be optimal for:',
      options: {
        A: 'Any summarization task regardless of requirements',
        B: 'Tasks where maximum quality is critical and token cost is secondary',
        C: 'High-volume, low-stakes summarization tasks',
        D: 'Testing atomic prompt limitations'
      },
      correct: 'B',
      hint: 'Consider when you\'d accept lower efficiency for higher absolute quality.',
      explanation: 'Version C trades efficiency for higher absolute quality (8/10). This makes sense for high-stakes tasks where the quality difference matters more than token costs—like legal document summarization or critical business analysis where accuracy is paramount.'
    }
  ]}
/>

## Key Takeaways

1. **Atomic prompts** are the fundamental unit of LLM interaction
2. They follow a basic structure: task + constraints + output format
3. They have inherent limitations: no memory, examples, or reasoning scaffolds
4. Even simple atomic prompts leverage the model's implicit knowledge
5. There's a power law relationship between context tokens and quality
6. Moving beyond atoms is the first step toward context engineering

## Next Steps

In the next section, we'll explore how to combine atoms into **molecules** — few-shot learning patterns that dramatically improve reliability and control.

[Continue to Module 2: Context Expansion →](/module-2)

---

## Deeper Dive: Prompt Templates

For those wanting to experiment more with atomic prompts, here are some templates to try:

```
# Basic instruction
{task}

# Persona-based
As a {persona}, {task}

# Format-specific
{task}
Format: {format_specification}

# Constraint-based
{task}
Constraints:
- {constraint_1}
- {constraint_2}
- {constraint_3}

# Step-by-step guided
{task}
Please follow these steps:
1. {step_1}
2. {step_2}
3. {step_3}
```

Try measuring the token count and quality for each template applied to the same task!

<QuizEmbed 
  moduleId="module-1" 
  section="final-assessment"
  quizzes={[
    {
      difficulty: 'hard',
      question: 'Which prompt template would be most effective for a complex analytical task requiring structured reasoning?',
      options: {
        A: 'Basic instruction template',
        B: 'Persona-based template',
        C: 'Step-by-step guided template',
        D: 'Format-specific template'
      },
      correct: 'C',
      hint: 'Consider which template provides the most structure for breaking down complexity.',
      explanation: 'The step-by-step guided template is most effective for complex analytical tasks because it provides explicit reasoning scaffolds, breaking down the task into manageable steps and guiding the model through a structured thought process.'
    },
    {
      difficulty: 'hard',
      question: 'Understanding the complete module, what is the primary reason we need to progress beyond atomic prompts to molecules and cells?',
      options: {
        A: 'Atomic prompts are too simple to be useful',
        B: 'To overcome fundamental limitations like lack of memory, examples, and reasoning structure',
        C: 'Because molecules and cells use fewer tokens',
        D: 'To make prompts more complex and impressive'
      },
      correct: 'B',
      hint: 'Think about all the limitations discussed and how molecules/cells address them.',
      explanation: 'We progress beyond atomic prompts specifically to address their fundamental limitations: lack of memory across interactions, absence of demonstration examples, no complex reasoning scaffolds, high output variance, and ambiguity. Molecules and cells provide these missing capabilities.'
    },
    {
      difficulty: 'hard',
      question: 'The transition from "prompt engineering" to "context engineering" represents:',
      options: {
        A: 'Just a change in terminology with no practical difference',
        B: 'A shift from crafting individual instructions to architecting entire information environments',
        C: 'Using more tokens in every prompt',
        D: 'Focusing only on technical implementation details'
      },
      correct: 'B',
      hint: 'Consider the difference between engineering a single prompt vs. engineering an entire context.',
      explanation: 'Context engineering represents a paradigm shift from crafting individual prompts to architecting entire information environments. It encompasses memory, examples, state management, multi-agent systems, and deliberate shaping of the context window—going far beyond single-instruction optimization.'
    }
  ]}
/>

---

## Attribution

**Content adapted from:**  
[Context-Engineering](https://github.com/davidkimai/Context-Engineering) by davidkimai  
Licensed under MIT License  

**Original Source:** `00_foundations/01_atoms_prompting.md`  
**Copyright:** © 2025 davidkimai
